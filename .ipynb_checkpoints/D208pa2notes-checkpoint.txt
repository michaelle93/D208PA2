linear regression:
mld_churn_vs_recency_lm = ols('has_churned ~ time_since_last_purchase',
				data=churn).fit()

print(mld_churn_vs_recency_lm.params)


intercept, slope= mld_churn_vs_recency_lm.params


LOGISTIC REGRESSION MODEL:

*using logit() function

from statsmodels.formula.api import logit
mld_churn_vs_recency_logit = logit('has_churned ~ time_since_last_purchase'
					data=churn).fit()
print(mld_churn_vs_recency_logit.params)

*visualizing logistic model

sns.regplot(x='time_since_last_purchase',
		y='has_churned',
		data=churn,
		ci=None,
		logistic=True)

plt.axline(xy1=(0,intercept),
		slope=slope,
		color='black')
plt.show()


*make prediciton with regression logistic model

*make a dataframe of explanatory variable values
 
explanatory_data = pd.DataFrame(
	{'time_since_last_purchase': np.arrange(-1, 6.25, 0.25)})

*add response column calculated using the predict method

prediction_data = explanatory_data.assign(
		has_churned = mdl_recency.predict(explanatory_data))

*addign point predictions

sns.regplot(x='time_since_last_purchase',
		y='has_churned',
		data=churn,
		ci=None,
		logistic=True)
sns.scatterplot(x='time_since_last_purchase',
		y='has_churned',
		data=prediction_data,
		color='red')



* getting the most likely outcome using the round function

prediction_data['most_likely_outcome'] = np.round(prediction_data['has_churned'])

* visualizing most likely outcome

sns.regplot(x='time_since_last_purchase',
		y='has_churned',
		data=churn,
		ci=None,
		logistic=True)
sns.scatterplot(x='time_since_last_purchase',
		y='most_likely_outcome',
		data=prediction_data,
		color='red')


*CONFUSION MATRIX: COUNTS OF OUTCOMES

actual_response = churn['has_churned']

predicted_response = np.round(mld_recency.predict())

outcomes = pd.DataFrame({'actual_response': actual_response,
			'predicted_reponse': predicted_reponse})

print(outcomes.value_counts(sort=False)

*Another way to create a confusion matrix automatically

conf_matrix = mld_recency.pred_table()

print(conf_matrix)

		0 	1    
	0	TN	FP
	1	FN	TP

from statsmodels.graphics.mosaicplot
import mosaic
mosaic(conf_matrix)

*Accuracy, proportion of correct predictions

acc =(TN+TP)/(TN+FN+FP+TP)
print(acc)
TN = conf_matrix[0,0]
TP = conf_matrix[1,1]
FN = conf_matrix[1,0]
FP = conf_matrix[0,1]

*Sensitivity, proportion of true positives

sens = TP/ (FN+TP)
print(sens)

*Specificity, proportion of true negatives

spec = TN/(TN+FP)
print(spec)


*MULTIPLE LOGISTIC REGRESSION

logit('response ~ explanatory', data=dataset).fit()  (SIMPLE LOGISTIC REGRESSION)

logit('response ~ explanatory1 + explanatory2', data=dataset).fit()  (MULTIPLE LOGISTIC '+' TO IGNORE INTERACTION) 
logit('response ~ explanatory1 * explanatory2', data=dataset).fit()  (MULTIPLE LOGISTIC '*' TO INCLUDE INTERACTIONS)

*CONFUSION MATRIX MULTIPLE VARIABLES

conf_matrix = mdl_logit.pred_table()
print(conf_matrix)

* prediction flow

from itertools import product

explanatory1 = some_values
explanatory2 = some_values

p = product(explanatory1, explanatory2)
explanatory_data = pd.DataFrame(p,
				columns =['explanatory1',
					'explanatory2'])
prediction_data = explanatory_data.assign(
	mass_g = mdl_logit.predict(explanatory_data))

* visualization

prediction_data['most_likely_outcome'] = np.round(prediction_data['has_churned])

sns.scatterplot(....x=.. y=..
		data=churn,
		hue='has_churned',
		...)
sns.scatterplot(...x=.. y=..
		data=prediction_data,
		hue='most_likely_outcome',
		...)


*LOGISTIC DISTRIBUTION
